{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d547093-5330-4950-af87-fd882f82e07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, log_loss, average_precision_score,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fbcca8-809c-4542-a816-79986c40358f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Load and Feature Engineering (No Leakage)\n",
    "# =========================\n",
    "def load_data(raw_path, user_path, ad_path):\n",
    "    raw_df = pd.read_csv(raw_path)\n",
    "    user_df = pd.read_csv(user_path)\n",
    "    ad_df = pd.read_csv(ad_path)\n",
    "    user_df.columns = user_df.columns.str.strip()\n",
    "    merged_df = raw_df.merge(user_df, how='left', left_on='user', right_on='userid')\n",
    "    merged_df = merged_df.merge(ad_df, how='left', on='adgroup_id')\n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2bd084-294b-42b3-9767-6b1d2db9afdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_ctr_stats_no_leakage(df):\n",
    "    df = df.sort_values('time_stamp')\n",
    "    df['user_total_clicks'] = df.groupby('user')['clk'].shift().fillna(0).groupby(df['user']).cumsum()\n",
    "    df['user_total_interactions'] = df.groupby('user').cumcount()\n",
    "    df['user_ctr'] = df['user_total_clicks'] / (df['user_total_interactions'] + 1e-6)\n",
    "    df['ad_total_clicks'] = df.groupby('adgroup_id')['clk'].shift().fillna(0).groupby(df['adgroup_id']).cumsum()\n",
    "    df['ad_total_impressions'] = df.groupby('adgroup_id').cumcount()\n",
    "    df['ad_ctr'] = df['ad_total_clicks'] / (df['ad_total_impressions'] + 1e-6)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c257235-6634-4922-81c0-4162ee0c2d2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# ADDITION: Enhanced behavior data loading with more sophisticated features\n",
    "# =========================\n",
    "def load_behavior_data(behavior_path, user_list, seq_len=50):  # Increased sequence length\n",
    "    behavior_df = pd.read_csv(behavior_path)\n",
    "    behavior_df['time_stamp'] = pd.to_datetime(behavior_df['time_stamp'], unit='s')\n",
    "    behavior_df = behavior_df[behavior_df['user'].isin(user_list)]\n",
    "    \n",
    "    # Enhanced categorical encoding\n",
    "    cate2idx = {c: i+1 for i, c in enumerate(behavior_df['cate'].dropna().unique())}\n",
    "    brand2idx = {b: i+1 for i, b in enumerate(behavior_df['brand'].dropna().unique())}\n",
    "    cate2idx['<PAD>'] = 0\n",
    "    brand2idx['<PAD>'] = 0\n",
    "    \n",
    "    behavior_df = behavior_df.sort_values(['user', 'time_stamp'])\n",
    "    user2seq = {}\n",
    "    \n",
    "    for uid, group in behavior_df.groupby('user'):\n",
    "        seq = [(cate2idx.get(row['cate'], 0), brand2idx.get(row['brand'], 0)) for _, row in group.iterrows()]\n",
    "        seq = seq[-seq_len:]  # Take last seq_len items\n",
    "        while len(seq) < seq_len:\n",
    "            seq.insert(0, (0, 0))  # Pad at beginning\n",
    "        user2seq[uid] = seq\n",
    "    \n",
    "    behavior_sequences = [user2seq.get(uid, [(0, 0)] * seq_len) for uid in user_list]\n",
    "    return torch.tensor(behavior_sequences, dtype=torch.long), len(cate2idx), len(brand2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36438d9-a537-4a98-a729-eae80c6ea0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df['time_stamp'] = pd.to_datetime(df['time_stamp'], unit='s')\n",
    "    df['hour'] = df['time_stamp'].dt.hour\n",
    "    df['day_of_week'] = df['time_stamp'].dt.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Add time-based features\n",
    "    df['is_morning'] = ((df['hour'] >= 6) & (df['hour'] < 12)).astype(int)\n",
    "    df['is_afternoon'] = ((df['hour'] >= 12) & (df['hour'] < 18)).astype(int)\n",
    "    df['is_evening'] = ((df['hour'] >= 18) & (df['hour'] < 24)).astype(int)\n",
    "    df['is_night'] = ((df['hour'] >= 0) & (df['hour'] < 6)).astype(int)\n",
    "    \n",
    "    df = add_ctr_stats_no_leakage(df)\n",
    "    \n",
    "    # Enhanced outlier handling and feature scaling\n",
    "    for col in ['price', 'user_total_interactions', 'ad_total_impressions']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].clip(upper=df[col].quantile(0.99))\n",
    "    \n",
    "    # Fill missing values more intelligently\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'unknown')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3bfa309-2d32-488c-8417-67980a02321c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    user_profile_features = [\n",
    "        'cms_segid', 'cms_group_id', 'final_gender_code', 'age_level',\n",
    "        'pvalue_level', 'shopping_level', 'occupation', 'new_user_class_level']\n",
    "    time_features = ['hour', 'day_of_week', 'is_weekend', 'is_morning', 'is_afternoon', 'is_evening', 'is_night']\n",
    "    user_behavior_features = ['user_total_interactions', 'user_ctr']\n",
    "    ad_features = ['adgroup_id', 'cate_id', 'campaign_id', 'customer', 'brand']\n",
    "    ad_performance_features = ['ad_total_impressions', 'ad_ctr']\n",
    "    features = user_profile_features + time_features + user_behavior_features + ad_features + ad_performance_features + ['price']\n",
    "    \n",
    "    y = df['clk']\n",
    "    X = df[features].copy()\n",
    "    cat_features = ad_features + user_profile_features\n",
    "    num_features = time_features + user_behavior_features + ad_performance_features + ['price']\n",
    "    \n",
    "    X_cat = X[cat_features].copy()\n",
    "    X_num = X[num_features].copy()\n",
    "    \n",
    "    # Enhanced scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(X_num), columns=X_num.columns, index=X.index)\n",
    "    \n",
    "    # Enhanced encoding with handling unknown categories\n",
    "    encoders = {}\n",
    "    for col in X_cat.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_cat[col] = le.fit_transform(X_cat[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    \n",
    "    X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test = train_test_split(\n",
    "        X_cat, X_num_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    cat_dims = [X_cat[col].nunique() for col in X_cat.columns]\n",
    "    return X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test, cat_dims, X_num_scaled.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c63222c-1432-4b89-83a5-e286e910b5d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Enhanced Transformer-based CTR Model\n",
    "# =========================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        Q = self.w_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        output = self.w_o(context)\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc2700b-b0f2-4fc2-bd36-a492a4111da8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_output, _ = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0a9e10-92c9-407f-b65e-baa641262485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CTRModelWithTransformer(nn.Module):\n",
    "    def __init__(self, cat_dims, emb_dim, num_features, hidden_dims, \n",
    "                 cate_vocab_size, brand_vocab_size, behavior_emb_dim=32, \n",
    "                 seq_len=50, n_heads=8, n_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding for categorical features\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(dim + 2, emb_dim) for dim in cat_dims])\n",
    "        self.num_linear = nn.Linear(num_features, emb_dim)\n",
    "\n",
    "        # Embedding for behavior sequence (cate, brand)\n",
    "        self.cate_emb = nn.Embedding(cate_vocab_size, behavior_emb_dim)\n",
    "        self.brand_emb = nn.Embedding(brand_vocab_size, behavior_emb_dim)\n",
    "        self.seq_len = seq_len\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(seq_len, behavior_emb_dim * 2))\n",
    "\n",
    "        # Transformer layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock(behavior_emb_dim * 2, n_heads, behavior_emb_dim * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.behavior_proj = nn.Linear(behavior_emb_dim * 2, emb_dim)\n",
    "\n",
    "        # Feature dimensions (categorical + numerical + behavior)\n",
    "        total_input_dim = emb_dim * (len(cat_dims) + 2)  # +2 for num and behavior\n",
    "        mlp_input_dim = total_input_dim * 2  # concat(x0, xl)\n",
    "\n",
    "        # Cross Network\n",
    "        self.cross_net = nn.ModuleList([\n",
    "            nn.Linear(total_input_dim, total_input_dim) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        # Deep MLP\n",
    "        layers = []\n",
    "        input_dim = mlp_input_dim\n",
    "        for h in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = h\n",
    "\n",
    "        # Final prediction layer\n",
    "        layers.extend([\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout / 2),\n",
    "            nn.Linear(input_dim // 2, 1)\n",
    "        ])\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, 0, 0.1)\n",
    "\n",
    "    def forward(self, x_cat, x_num, behavior_seq):\n",
    "        # Categorical feature embedding\n",
    "        cat_embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        cat_emb = torch.cat(cat_embeds, dim=1)\n",
    "\n",
    "        # Numeric feature projection\n",
    "        num_emb = self.num_linear(x_num)\n",
    "\n",
    "        # Behavior sequence embedding + position\n",
    "        cate_seq = self.cate_emb(behavior_seq[:, :, 0])\n",
    "        brand_seq = self.brand_emb(behavior_seq[:, :, 1])\n",
    "        behavior_emb = torch.cat([cate_seq, brand_seq], dim=-1)\n",
    "        behavior_emb = behavior_emb + self.pos_encoding.unsqueeze(0)\n",
    "\n",
    "        # Attention mask\n",
    "        mask = (behavior_seq[:, :, 0] != 0).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "        for transformer in self.transformer_layers:\n",
    "            behavior_emb = transformer(behavior_emb, mask)\n",
    "\n",
    "        # Weighted pooling\n",
    "        seq_mask = (behavior_seq[:, :, 0] != 0).float().unsqueeze(-1)\n",
    "        behavior_repr = (behavior_emb * seq_mask).sum(dim=1) / (seq_mask.sum(dim=1) + 1e-8)\n",
    "        behavior_repr = self.behavior_proj(behavior_repr)\n",
    "\n",
    "        # Combine all features\n",
    "        all_features = torch.cat([cat_emb, num_emb, behavior_repr], dim=1)\n",
    "\n",
    "        # Cross Network\n",
    "        x0 = all_features\n",
    "        xl = all_features\n",
    "        for cross_layer in self.cross_net:\n",
    "            xl = x0 * cross_layer(xl) + xl\n",
    "\n",
    "        # Combine cross and raw\n",
    "        final_features = torch.cat([x0, xl], dim=1)\n",
    "\n",
    "        # Prediction\n",
    "        output = self.mlp(final_features)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a29725-5783-45df-ba32-ef622d8bf506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Enhanced Training with Advanced Techniques\n",
    "# =========================\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, X_cat, X_num, behavior_seq, y):\n",
    "        self.X_cat = torch.tensor(X_cat.values, dtype=torch.long)\n",
    "        self.X_num = torch.tensor(X_num.values, dtype=torch.float32)\n",
    "        self.behavior_seq = behavior_seq\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_cat[idx], self.X_num[idx], self.behavior_seq[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fbb5bf-2928-4ead-adff-5feca6b93cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_with_behavior_advanced(X_cat_train, X_cat_test, X_num_train, X_num_test, \n",
    "                               behavior_train, behavior_test, y_train, y_test, \n",
    "                               cat_dims, num_features, cate_vocab_size, brand_vocab_size,\n",
    "                               emb_dim=64, hidden_dims=[512, 256, 128], batch_size=1024, \n",
    "                               epochs=50, lr=0.001, weight_decay=1e-5):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Enhanced model architecture\n",
    "    model = CTRModelWithTransformer(\n",
    "        cat_dims, emb_dim, num_features, hidden_dims, \n",
    "        cate_vocab_size, brand_vocab_size, \n",
    "        behavior_emb_dim=48, seq_len=50, n_heads=8, n_layers=4, dropout=0.15\n",
    "    ).to(device)\n",
    "    \n",
    "    # Datasets and loaders\n",
    "    train_dataset = CTRDataset(X_cat_train, X_num_train, behavior_train, y_train)\n",
    "    test_dataset = CTRDataset(X_cat_test, X_num_test, behavior_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Advanced training setup\n",
    "    pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()], dtype=torch.float32).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # AdamW optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=10, T_mult=2, eta_min=lr/10\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    best_auc = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        \n",
    "        for x_cat, x_num, beh, y in train_loader:\n",
    "            x_cat, x_num, beh, y = x_cat.to(device), x_num.to(device), beh.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_cat, x_num, beh)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            train_preds.extend(torch.sigmoid(logits).cpu().detach().numpy())\n",
    "            train_targets.extend(y.cpu().numpy())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_cat, x_num, beh, y in test_loader:\n",
    "                x_cat, x_num, beh = x_cat.to(device), x_num.to(device), beh.to(device)\n",
    "                y = y.to(device)\n",
    "                logits = model(x_cat, x_num, beh)\n",
    "                val_loss += criterion(logits, y).item()\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                y_pred.extend(probs)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_auc = roc_auc_score(train_targets, train_preds)\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        logloss = log_loss(y_test, y_pred)\n",
    "        avg_precision = average_precision_score(y_test, y_pred)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {total_loss/len(train_loader):.4f} - \"\n",
    "              f\"Val Loss: {val_loss/len(test_loader):.4f} - \"\n",
    "              f\"Train AUC: {train_auc:.4f} - \"\n",
    "              f\"Val AUC: {auc:.4f} - \"\n",
    "              f\"AP: {avg_precision:.4f} - \"\n",
    "              f\"LogLoss: {logloss:.4f} - \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"★ New best AUC: {best_auc:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, best_auc, logloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766db0f5-cda1-40be-8b5e-8f28b8b2b3b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data...\n",
      " Preparing features...\n",
      " Dataset statistics:\n",
      "  Training samples: 21,246,368\n",
      "  Test samples: 5,311,593\n",
      "  Positive rate: 0.0514\n",
      "  Categorical features: 13\n",
      "  Numerical features: 12\n",
      " Loading behavior data...\n",
      "  Behavior sequence length: 50\n",
      "  Category vocabulary: 9874\n",
      "  Brand vocabulary: 203528\n",
      " Training Enhanced Transformer CTR model...\n",
      "Using device: cuda\n",
      "Model parameters: 147,227,201\n",
      "Epoch 1/20 - Train Loss: 1.2445 - Val Loss: 1.2367 - Train AUC: 0.6530 - Val AUC: 0.6632 - AP: 0.0959 - LogLoss: 0.6061 - LR: 0.001956\n",
      "★ New best AUC: 0.6632\n",
      "Epoch 2/20 - Train Loss: 1.1981 - Val Loss: 1.2118 - Train AUC: 0.6935 - Val AUC: 0.6820 - AP: 0.1022 - LogLoss: 0.6129 - LR: 0.001828\n",
      "★ New best AUC: 0.6820\n",
      "Epoch 3/20 - Train Loss: 1.1331 - Val Loss: 1.2145 - Train AUC: 0.7354 - Val AUC: 0.6863 - AP: 0.1012 - LogLoss: 0.5981 - LR: 0.001629\n",
      "★ New best AUC: 0.6863\n",
      "Epoch 4/20 - Train Loss: 1.0839 - Val Loss: 1.2286 - Train AUC: 0.7637 - Val AUC: 0.6828 - AP: 0.0973 - LogLoss: 0.6041 - LR: 0.001378\n",
      "Epoch 6/20 - Train Loss: 0.9823 - Val Loss: 1.3565 - Train AUC: 0.8125 - Val AUC: 0.6869 - AP: 0.1008 - LogLoss: 0.5674 - LR: 0.000822\n",
      "★ New best AUC: 0.6869\n",
      "Epoch 7/20 - Train Loss: 0.9326 - Val Loss: 1.3999 - Train AUC: 0.8328 - Val AUC: 0.6868 - AP: 0.0998 - LogLoss: 0.5791 - LR: 0.000571\n",
      "Epoch 8/20 - Train Loss: 0.8867 - Val Loss: 1.4693 - Train AUC: 0.8502 - Val AUC: 0.6870 - AP: 0.0998 - LogLoss: 0.5375 - LR: 0.000372\n",
      "★ New best AUC: 0.6870\n",
      "Epoch 10/20 - Train Loss: 0.8187 - Val Loss: 1.7447 - Train AUC: 0.8733 - Val AUC: 0.6805 - AP: 0.0967 - LogLoss: 0.5585 - LR: 0.002000\n",
      "Epoch 11/20 - Train Loss: 0.9688 - Val Loss: 1.3451 - Train AUC: 0.8188 - Val AUC: 0.6866 - AP: 0.0975 - LogLoss: 0.5934 - LR: 0.001989\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Main Function with Enhanced Configuration\n",
    "# =========================\n",
    "def main():\n",
    "    print(\" Loading data...\")\n",
    "    df = load_data('raw_sample.csv', 'user_profile.csv', 'ad_feature.csv')\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    print(\" Preparing features...\")\n",
    "    X_cat_train, X_cat_test, X_num_train, X_num_test, y_train, y_test, cat_dims, num_features = prepare_features(df)\n",
    "    \n",
    "    print(\" Dataset statistics:\")\n",
    "    print(f\"  Training samples: {len(X_cat_train):,}\")\n",
    "    print(f\"  Test samples: {len(X_cat_test):,}\")\n",
    "    print(f\"  Positive rate: {y_train.mean():.4f}\")\n",
    "    print(f\"  Categorical features: {len(cat_dims)}\")\n",
    "    print(f\"  Numerical features: {num_features}\")\n",
    "    \n",
    "    # Build behavior data only for users in df\n",
    "    print(\" Loading behavior data...\")\n",
    "    full_user_ids = df['user'].tolist()\n",
    "    behavior_tensor, cate_vocab_size, brand_vocab_size = load_behavior_data('behavior_log.csv', full_user_ids, seq_len=50)\n",
    "    \n",
    "    print(f\"  Behavior sequence length: 50\")\n",
    "    print(f\"  Category vocabulary: {cate_vocab_size}\")\n",
    "    print(f\"  Brand vocabulary: {brand_vocab_size}\")\n",
    "    \n",
    "    # Align behavior_tensor to X_cat index (fixed alignment)\n",
    "    behavior_tensor = pd.Series(list(behavior_tensor.numpy()), index=df.index)\n",
    "    behavior_train = torch.tensor(behavior_tensor.loc[X_cat_train.index].tolist(), dtype=torch.long)\n",
    "    behavior_test = torch.tensor(behavior_tensor.loc[X_cat_test.index].tolist(), dtype=torch.long)\n",
    "    \n",
    "    print(\" Training Enhanced Transformer CTR model...\")\n",
    "    model, auc, logloss = train_with_behavior_advanced(\n",
    "        X_cat_train, X_cat_test, X_num_train, X_num_test,\n",
    "        behavior_train, behavior_test,\n",
    "        y_train, y_test,\n",
    "        cat_dims, num_features,\n",
    "        cate_vocab_size, brand_vocab_size,\n",
    "        emb_dim=80,  # Increased embedding dimension\n",
    "        hidden_dims=[1024, 512, 256, 128],  # Deeper network\n",
    "        batch_size=512,  # Smaller batch for better gradients\n",
    "        epochs=20,  # More epochs with early stopping\n",
    "        lr=0.002,  # Slightly higher learning rate\n",
    "        weight_decay=1e-4  # Stronger regularization\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Training completed!\")\n",
    "    print(f\" Final AUC: {auc:.6f}\")\n",
    "    print(f\" Final LogLoss: {logloss:.6f}\")\n",
    "    \n",
    "    return model, auc, logloss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75d8782c-077a-43b3-85f4-0b91b9018bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                       Type        Data/Info\n",
      "----------------------------------------------------\n",
      "CTRDataset                     type        <class '__main__.CTRDataset'>\n",
      "CTRModelWithTransformer        type        <class '__main__.CTRModelWithTransformer'>\n",
      "DataLoader                     type        <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Dataset                        type        <class 'torch.utils.data.dataset.Dataset'>\n",
      "F                              module      <module 'torch.nn.functio<...>/torch/nn/functional.py'>\n",
      "LabelEncoder                   type        <class 'sklearn.preproces<...>ing._label.LabelEncoder'>\n",
      "MultiHeadAttention             type        <class '__main__.MultiHeadAttention'>\n",
      "StandardScaler                 type        <class 'sklearn.preproces<...>ng._data.StandardScaler'>\n",
      "TransformerBlock               type        <class '__main__.TransformerBlock'>\n",
      "accuracy_score                 function    <function accuracy_score at 0x7f01cf5bdbd0>\n",
      "add_ctr_stats_no_leakage       function    <function add_ctr_stats_n<...>eakage at 0x7f02e1d388b0>\n",
      "average_precision_score        function    <function average_precisi<...>_score at 0x7f01cf5e1000>\n",
      "f1_score                       function    <function f1_score at 0x7f01cf5be3b0>\n",
      "feature_engineering            function    <function feature_engineering at 0x7f01cf0564d0>\n",
      "load_behavior_data             function    <function load_behavior_data at 0x7f01cf0560e0>\n",
      "load_data                      function    <function load_data at 0x7f02e1d38160>\n",
      "log_loss                       function    <function log_loss at 0x7f01cf5bef80>\n",
      "main                           function    <function main at 0x7f01cf08c040>\n",
      "math                           module      <module 'math' from '/opt<...>310-x86_64-linux-gnu.so'>\n",
      "nn                             module      <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\n",
      "np                             module      <module 'numpy' from '/op<...>kages/numpy/__init__.py'>\n",
      "pd                             module      <module 'pandas' from '/o<...>ages/pandas/__init__.py'>\n",
      "prepare_features               function    <function prepare_features at 0x7f01cf0568c0>\n",
      "roc_auc_score                  function    <function roc_auc_score at 0x7f01cf5e12d0>\n",
      "torch                          module      <module 'torch' from '/op<...>kages/torch/__init__.py'>\n",
      "train_test_split               function    <function train_test_split at 0x7f01cf640e50>\n",
      "train_with_behavior_advanced   function    <function train_with_beha<...>vanced at 0x7f01cf08c1f0>\n",
      "warnings                       module      <module 'warnings' from '<...>/python3.10/warnings.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcc028-fe7a-452e-bfab-39438238672a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
